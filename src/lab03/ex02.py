import re


def tokenize(text: str) -> list[str]:
    """
    Разбивает текст на слова (токены).

    Находит последовательности букв, цифр, подчеркиваний,
    может содержать дефисы внутри слов.

    Args:
        text: Текст для разбиения на слова

    Returns:
        Список найденных слов

    Examples:
        >>> tokenize("привет, мир!")
        ['привет', 'мир']

        >>> tokenize("по-настоящему круто")
        ['по-настоящему', 'круто']

        >>> tokenize("emoji  не слово")
        ['emoji', 'не', 'слово']
    """
    # Регулярное выражение для поиска слов
    # [\w]+    - одна или больше букв/цифр/подчеркиваний
    # (?:-[\w]+)* - ноль или больше повторов: дефис + буквы/цифры
    pattern = r"[\w]+(?:-[\w]+)*"

    # Находим все совпадения с шаблоном
    tokens = re.findall(pattern, text)

    return tokens

