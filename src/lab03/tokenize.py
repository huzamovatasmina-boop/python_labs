import re


def tokenize(text: str) -> list[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞ (—Ç–æ–∫–µ–Ω—ã).

    –ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±—É–∫–≤, —Ü–∏—Ñ—Ä, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–π,
    –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –¥–µ—Ñ–∏—Å—ã –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤.

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —Å–ª–æ–≤–∞

    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤

    Examples:
        >>> tokenize("–ø—Ä–∏–≤–µ—Ç, –º–∏—Ä!")
        ['–ø—Ä–∏–≤–µ—Ç', '–º–∏—Ä']

        >>> tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ")
        ['–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É', '–∫—Ä—É—Ç–æ']

        >>> tokenize("emoji üêà‚Äç‚¨õ –Ω–µ —Å–ª–æ–≤–æ")
        ['emoji', '–Ω–µ', '—Å–ª–æ–≤–æ']
    """
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤
    # [\w]+    - –æ–¥–Ω–∞ –∏–ª–∏ –±–æ–ª—å—à–µ –±—É–∫–≤/—Ü–∏—Ñ—Ä/–ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–π
    # (?:-[\w]+)* - –Ω–æ–ª—å –∏–ª–∏ –±–æ–ª—å—à–µ –ø–æ–≤—Ç–æ—Ä–æ–≤: –¥–µ—Ñ–∏—Å + –±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã
    pattern = r"[\w]+(?:-[\w]+)*"

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å —à–∞–±–ª–æ–Ω–æ–º
    tokens = re.findall(pattern, text)

    return tokens

print("=== –¢–µ—Å—Ç—ã —Ç–µ–∫—Å—Ç–∞ ===")
texts = [
    "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä",
    "hello,world!!!", 
    "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ",
    "2025 –≥–æ–¥",
    "emoji üêà‚Äç‚¨õ –Ω–µ —Å–ª–æ–≤–æ"
]

for text in texts:
    result = tokenize(text)
    print(result)